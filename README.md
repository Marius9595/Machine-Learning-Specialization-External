# [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction#courses)
#BreakIntoAI with Machine Learning Specialization. Master fundamental AI concepts and develop practical machine learning skills in the beginner-friendly, 3-course program by AI visionary Andrew Ng

## [Course 1: Supervised Machine Learning: Regression and Classification](https://www.coursera.org/learn/machine-learning?specialization=machine-learning-introduction)
If you want to break into cutting-edge AI, this course will help you do so. Deep learning engineers are highly sought after, and mastering deep learning will give you numerous new career opportunities. Deep learning is also a new “superpower” that will let you build AI systems that just weren’t possible a few years ago.

### In the first course of the Machine Learning Specialization, you will:
- Build machine learning models in Python using popular machine learning libraries NumPy and scikit-learn.
- Build and train supervised machine learning models for prediction and binary classification tasks, including linear regression and logistic regression

### [Week 1](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course1/Week1)
- Learn the difference between supervised and unsupervised learning and regression and classification tasks.
- Build a linear regression model.
- Implement and understand the purpose of a cost function.
- Implement and understand how gradient descent is used to train a machine learning model.

#### Optional Lab
> #### [1. Python and Jupyter Notebooks](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week1/C1_W1_Lab01_Python_Jupyter_Soln.ipynb)
>>- In this lab, you will explore some of the tools that are used in this course. Python and Jupyter notebooks.
> #### [2. Model representation](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week1/C1_W1_Lab03_Model_Representation_Soln.ipynb)
>>- In this ungraded lab, you can see how a linear regression model is defined in code, and you can see plots that show how well a model fits some data given choices of w and b.  You can also try different values of w and b to see if it improves the fit to the data.
> #### [3. Cost function](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week1/C1_W1_Lab04_Cost_function_Soln.ipynb)
>>- This optional lab will show you how the cost function is implemented in code. And given a small training set and different choices for the parameters you’ll be able to see how the cost varies depending on how well the model fits the data. 
>>- In the optional lab, you'll also get to play with an interactive contour plot.  You can use your mouse cursor to click anywhere on the contour plot, and you see the straight line defined by the values you chose, for parameters w and b.
>>- Finally the optional lab also has a 3d surface plot that you can manually rotate and spin around, using your mouse cursor, to take a better look at what the cost function looks like.
> #### [4. Gradient descent](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week1/C1_W1_Lab05_Gradient_Descent_Soln.ipynb)
>>- In the optional lab, you’ll see a review of the gradient descent algorithm, as well as how to implement it in code.
>>- You will also see a plot that shows how the cost decreases as you continue training more iterations.  And you’ll also see a contour plot, seeing how the cost gets closer to the global minimum as gradient descent finds better and better values for the parameters w and b.

### [Week 2](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course1/Week2)
- Build and train a regression model that takes multiple features as input (multiple linear regression).
- Implement and understand the cost function and gradient descent for multiple linear regression.
- Implement and understand methods for improving machine learning models by choosing the learning rate, plotting the learning curve, performing feature engineering, and - applying polynomial regression.

#### Optional Lab
> #### [1. Python, NumPy and vectorization](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week2/Optional%20Lab/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb)
>>- This optional lab will show you how to use NumPy to implement the math operations of creating vectors and matrices, and performing dot products and matrix multiplications in code.  These NumPy operations use vectorization behind the scenes to make the code run faster!
>>- This optional lab introduces a fair amount of new numpy syntax, so don't worry about understanding all of it right away.  But you can save this notebook and use it as a reference to look at when you’re working with data stored in numpy arrays.
> #### [2. Multiple linear regression](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week2/Optional%20Lab/C1_W2_Lab02_Multiple_Variable_Soln.ipynb)
>>- In this optional lab, you’ll see how to define a multiple regression model, in code, and also how to calculate the prediction, f of x.  You’ll also see how to calculate the cost, and implement gradient descent for a multiple linear regression model.
>>- This will be using Python’s numpy library, so if any of the code looks very new, please take a look at the previous optional lab that introduces Numpy and Vectorization, for a refresher of Numpy functions and how to implement those in code.
> #### [3. Feature scaling and learning rate](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week2/Optional%20Lab/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb)
>>- In this optional lab you can also take a look at how feature scaling is done in code and also see how different choices of the learning rate alpha can lead to better or worse training of your model.  This  will help you to gain a deeper intuition about feature scaling as well as the learning rate alpha.
> #### [4. Feature engineering and Polynomial regression](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week2/Optional%20Lab/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb)
>>- In this optional lab, you'll see some code that implements polynomial regression including features like x, x squared, and x cubed.
> #### [5. Linear regression with scikit-learn](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week2/Optional%20Lab/C1_W2_Lab05_Sklearn_GD_Soln.ipynb)
>>- This optional lab shows how to use a popular open source toolkit that implements linear regression. Scikit learn is a very widely used open source machine learning library that is used by many practitioners in many of the top AI, internet, machine learning companies in the world.
>>- So if either now or in the future you are using machine learning in in your job there’s a very good chance you use tools like Scikit learn to train your models. 

### [Week 3](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course1/Week3)
- Implement and understand the logistic regression model for classification.
- Learn why logistic regression is better suited for classification tasks than the linear regression model is.
- Implement and understand the cost function and gradient descent for logistic regression.
- Understand the problem of “overfitting” and improve model performance using regularization.
- Implement regularization to improve both regression and classification models.

## [Course 2: Advanced Learning Algorithms](https://www.coursera.org/learn/advanced-learning-algorithms?specialization=machine-learning-introduction)
In the second course of the Machine Learning Specialization, you will: Build and train a neural network with TensorFlow to perform multi-class classification. Apply best practices for machine learning development so that your models generalize…

### In the second course of the Machine Learning Specialization, you will:
- Build and train a neural network with TensorFlow to perform multi-class classification.
- Apply best practices for machine learning development so that your models generalize to data and tasks in the real world.
- Build and use decision trees and tree ensemble methods, including random forests and boosted trees.

### [Week 1](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course2/Week1)
- Build a neural network for binary classification of handwritten digits using TensorFlow.
- Gain a deeper understanding by implementing a neural network in Python from scratch.
- Optionally learn how neural network computations are “vectorized” to use parallel processing for faster training and prediction.

### [Week 2](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course2/Week2)
- Build a neural network to perform multi-class classification of handwritten digits in TensorFlow, using categorical cross-entropy loss functions and the softmax activation.
- Learn where to use different activation functions (ReLu, linear, sigmoid, softmax) in a neural network, depending on the task you want your model to perform.
- Use the advanced “Adam optimizer” to train your model more efficiently.

### [Week 3](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course2/Week3)
- Discover the value of separating your data set into training, cross-validation, and test sets.
- Choose from various versions of your model using a cross-validation dataset, and evaluate its ability to generalize to real-world data using a test dataset.
- Use “learning curves” to determine if your model is experiencing high bias or high variance (or both), and learn which techniques to apply (regularization, adding more data, adding or removing input features) to improve your model’s performance.
- Learn how the “bias-variance trade-off” is different in the age of deep learning, and apply Andrew Ng’s advice for handling bias and variance when training neural networks.
- Learn to apply the “iterative loop” of machine learning development to train, evaluate, and tune your model.
- Apply “data-centric AI” to not only tune your model but tune your data (using data synthesis or data augmentation) to improve your model’s performance.

### [Week 4](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course2/Week4)
- Build decision trees and tree ensembles, such as random forest and XGBoost (boosted trees) to make predictions.
- Learn when to use neural network or tree ensemble models for your task, as these are the two most commonly used supervised learning models in practice today.

## [Course 3: Unsupervised Learning, Recommenders, Reinforcement Learning](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning?specialization=machine-learning-introduction)
In the third course of the Machine Learning Specialization, you will: Use unsupervised learning techniques for unsupervised learning: including clustering and anomaly detection. Build recommender systems with a collaborative filtering approach and a content-based deep…

### In the third course of the Machine Learning Specialization, you will:
- Use unsupervised learning techniques for unsupervised learning: including clustering and anomaly detection.
- Build recommender systems with a collaborative filtering approach and a content-based deep learning method.
- Build a deep reinforcement learning model.

### [Week 1](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course3/Week1)
- Implement K-mean clustering.
- Implement anomaly detection.
- Learn how to choose between supervised learning or anomaly detection to solve certain tasks.

### [Week 2](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course3/Week2)
- Build a recommender system using collaborative filtering.
- Build a recommender system using a content-based deep learning method.

### [Week 3](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course3/Week3)
- Build a deep reinforcement learning model (Deep Q Network).
