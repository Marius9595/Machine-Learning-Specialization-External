# [Machine Learning Specialization](https://www.coursera.org/specializations/machine-learning-introduction#courses)
#BreakIntoAI with Machine Learning Specialization. Master fundamental AI concepts and develop practical machine learning skills in the beginner-friendly, 3-course program by AI visionary Andrew Ng

## [Course 1: Supervised Machine Learning: Regression and Classification](https://www.coursera.org/learn/machine-learning?specialization=machine-learning-introduction)
If you want to break into cutting-edge AI, this course will help you do so. Deep learning engineers are highly sought after, and mastering deep learning will give you numerous new career opportunities. Deep learning is also a new “superpower” that will let you build AI systems that just weren’t possible a few years ago.

### In the first course of the Machine Learning Specialization, you will:
- Build machine learning models in Python using popular machine learning libraries NumPy and scikit-learn.
- Build and train supervised machine learning models for prediction and binary classification tasks, including linear regression and logistic regression

### [Week 1](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course1/Week1)
- Learn the difference between supervised and unsupervised learning and regression and classification tasks.
- Build a linear regression model.
- Implement and understand the purpose of a cost function.
- Implement and understand how gradient descent is used to train a machine learning model.

#### Optional Lab
> #### [1. Python and Jupyter Notebooks](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week1/C1_W1_Lab01_Python_Jupyter_Soln.ipynb)
>>- In this lab, you will explore some of the tools that are used in this course. Python and Jupyter notebooks.
> #### [2. Model representation](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week1/C1_W1_Lab03_Model_Representation_Soln.ipynb)
>>- In this lab, you can see how a linear regression model is defined in code, and you can see plots that show how well a model fits some data given choices of w and b.  You can also try different values of w and b to see if it improves the fit to the data.
> #### [3. Cost function](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week1/C1_W1_Lab04_Cost_function_Soln.ipynb)
>>- This optional lab will show you how the cost function is implemented in code. And given a small training set and different choices for the parameters you’ll be able to see how the cost varies depending on how well the model fits the data. 
>>- In the optional lab, you'll also get to play with an interactive contour plot.  You can use your mouse cursor to click anywhere on the contour plot, and you see the straight line defined by the values you chose, for parameters w and b.
>>- Finally the optional lab also has a 3d surface plot that you can manually rotate and spin around, using your mouse cursor, to take a better look at what the cost function looks like.
> #### [4. Gradient descent](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week1/C1_W1_Lab05_Gradient_Descent_Soln.ipynb)
>>- In the optional lab, you’ll see a review of the gradient descent algorithm, as well as how to implement it in code.
>>- You will also see a plot that shows how the cost decreases as you continue training more iterations.  And you’ll also see a contour plot, seeing how the cost gets closer to the global minimum as gradient descent finds better and better values for the parameters w and b.

### [Week 2](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course1/Week2)
- Build and train a regression model that takes multiple features as input (multiple linear regression).
- Implement and understand the cost function and gradient descent for multiple linear regression.
- Implement and understand methods for improving machine learning models by choosing the learning rate, plotting the learning curve, performing feature engineering, and - applying polynomial regression.

#### Optional Lab
> #### [1. Python, NumPy and vectorization](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week2/Optional%20Lab/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb)
>>- This optional lab will show you how to use NumPy to implement the math operations of creating vectors and matrices, and performing dot products and matrix multiplications in code.  These NumPy operations use vectorization behind the scenes to make the code run faster!
>>- This optional lab introduces a fair amount of new numpy syntax, so don't worry about understanding all of it right away.  But you can save this notebook and use it as a reference to look at when you’re working with data stored in numpy arrays.
> #### [2. Multiple linear regression](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week2/Optional%20Lab/C1_W2_Lab02_Multiple_Variable_Soln.ipynb)
>>- In this optional lab, you’ll see how to define a multiple regression model, in code, and also how to calculate the prediction, f of x.  You’ll also see how to calculate the cost, and implement gradient descent for a multiple linear regression model.
>>- This will be using Python’s numpy library, so if any of the code looks very new, please take a look at the previous optional lab that introduces Numpy and Vectorization, for a refresher of Numpy functions and how to implement those in code.
> #### [3. Feature scaling and learning rate](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week2/Optional%20Lab/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb)
>>- In this optional lab you can also take a look at how feature scaling is done in code and also see how different choices of the learning rate alpha can lead to better or worse training of your model.  This  will help you to gain a deeper intuition about feature scaling as well as the learning rate alpha.
> #### [4. Feature engineering and Polynomial regression](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week2/Optional%20Lab/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb)
>>- In this optional lab, you'll see some code that implements polynomial regression including features like x, x squared, and x cubed.
> #### [5. Linear regression with scikit-learn](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week2/Optional%20Lab/C1_W2_Lab05_Sklearn_GD_Soln.ipynb)
>>- This optional lab shows how to use a popular open source toolkit that implements linear regression. Scikit learn is a very widely used open source machine learning library that is used by many practitioners in many of the top AI, internet, machine learning companies in the world.
>>- So if either now or in the future you are using machine learning in in your job there’s a very good chance you use tools like Scikit learn to train your models. 

#### Practice Lab
> #### [Linear regression](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week2/Practice%20Lab/C1_W2_Linear_Regression.ipynb)
>>- In this notebook you will implement the regression model using gradient descent .This notebook will give you a brief introduction to Python. Even if you've used Python before, these exercises will help familiarize you with some essential functions.  

### [Week 3](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course1/Week3)
- Implement and understand the logistic regression model for classification.
- Learn why logistic regression is better suited for classification tasks than the linear regression model is.
- Implement and understand the cost function and gradient descent for logistic regression.
- Understand the problem of “overfitting” and improve model performance using regularization.
- Implement regularization to improve both regression and classification models.

#### Optional Lab
> #### [1. Classification](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week3/Optional%20Lab/C1_W3_Lab01_Classification_Soln.ipynb)
>>- In this optional lab, you’ll get to take a look at what happens when you try to use linear regression for classification on categorical data.  You can see an interactive plot that attempts to classify between two categories.  And you may notice that this doesn’t work very well, which is okay, because that motivates the need for a different model to do classification tasks.
> #### [2. Sigmoid function and logistic regression](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week3/Optional%20Lab/C1_W3_Lab02_Sigmoid_function_Soln.ipynb)
>>- In this optional lab, you’ll get to see how the sigmoid function is implemented in code.  You can see a plot that uses the sigmoid to improve the classification task that you saw in the previous optional lab.
> #### [3. Decision boundary](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week3/Optional%20Lab/C1_W3_Lab03_Decision_Boundary_Soln.ipynb)
>>- In this optional lab, you’ll get to see the code implementation of the decision boundary.  In this example, there will be two features, so you can see the decision boundary as a line.
> #### [4. Logistic loss](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week3/Optional%20Lab/C1_W3_Lab04_LogisticLoss_Soln.ipynb)
>>- In this optional lab, you’ll get to take a look at how the squared error cost doesn’t work very well for classification, because you’ll see a surface plot of a very wiggly cost surface with many local minima.
>>- Then, you’ll get to take a look at the new logistic loss function, and see that this produces a nice and smooth surface plot that does not have several local minima.
> #### [5. Cost function for logistic regression](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week3/Optional%20Lab/C1_W3_Lab05_Cost_Function_Soln.ipynb)
>>- This optional lab will show you how the logistic cost function is implemented in code.  You will get to implement this later in the practice lab.
>>- This optional lab also shows you how two different choices of the parameters will lead to different cost calculations.  So you can see in a plot that the better fitting decision boundary has a lower cost relative to another choice for a decision boundary.
> #### [6. Gradient descent for logistic regression](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week3/Optional%20Lab/C1_W3_Lab06_Gradient_Descent_Soln.ipynb)
>>- In this optional lab, you’ll take a look how the gradient for logistic cost is calculated in code.  This will be useful to look at because you will implement this in the practice lab.
>>- After you run gradient descent in the lab, there will be a nice set of animated plots that show gradient descent in action. You’ll see the sigmoid function, the contour plot of the cost, the 3D surface plot of the cost, and the learning curve all evolve as gradient descent runs.
> #### [7. Logistic regression with scikit-learn](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week3/Optional%20Lab/C1_W3_Lab07_Scikit_Learn_Soln.ipynb)
>>- This optional lab will show you how to use the popular sci-kit learn library to train a logistic regression model for classification.
> #### [8. Overfitting](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week3/Optional%20Lab/C1_W3_Lab08_Overfitting_Soln.ipynb)
>>- In this optional lab, you’ll take a look at some examples of overfitting that you can adjust by clicking on the options in the plot.  In the lab you’ll be able to add your own data points by clicking on the plot, and see how that changes the fitted curve.
>>- You can try examples for both regression and classification.  You can also change the degree of the polynomial to see how the curve either overfits or underfits the data.
> #### [9. Regularization](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week3/Optional%20Lab/C1_W3_Lab09_Regularization_Soln.ipynb)
>>- In this final optional lab of week 3, you’ll revisit overfitting, and in this interactive plot, you can now choose to regularize your models, both regression and classification, by enabling regularization during gradient descent by selecting a value for lambda.
>>- Please take a look at the code for implementing regularized logistic regression in particular, because you will implement this in the practice lab.

#### Practice Lab
> [Logistic regression](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course1/Week3/Practice%20Lab/C1_W3_Logistic_Regression.ipynb)
>>- You'll get to practice implementing logistic regression for binary classification.

## [Course 2: Advanced Learning Algorithms](https://www.coursera.org/learn/advanced-learning-algorithms?specialization=machine-learning-introduction)
In the second course of the Machine Learning Specialization, you will: Build and train a neural network with TensorFlow to perform multi-class classification. Apply best practices for machine learning development so that your models generalize…

### In the second course of the Machine Learning Specialization, you will:
- Build and train a neural network with TensorFlow to perform multi-class classification.
- Apply best practices for machine learning development so that your models generalize to data and tasks in the real world.
- Build and use decision trees and tree ensemble methods, including random forests and boosted trees.

### [Week 1](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course2/Week1)
- Build a neural network for binary classification of handwritten digits using TensorFlow.
- Gain a deeper understanding by implementing a neural network in Python from scratch.
- Optionally learn how neural network computations are “vectorized” to use parallel processing for faster training and prediction.

#### Optional Lab
> #### [1. Neurons and Layers](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course2/Week1/Optional%20Lab/C2_W1_Lab01_Neurons_and_Layers.ipynb)
>>- Examples of Neurons and Layers.
> #### [2. Coffee Roasting in Tensorflow](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course2/Week1/Optional%20Lab/C2_W1_Lab02_CoffeeRoasting_TF.ipynb)
>>- Implementing a neural network in tensorflow.
> #### [3. CoffeeRoastingNumPy](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course2/Week1/Optional%20Lab/C2_W1_Lab03_CoffeeRoasting_Numpy.ipynb)
>>- This lab demonstrates a neural network forwarding path in NumPy.

#### Practice Lab
> [Neural Networks for Binary Classification](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course2/Week1/Practice%20Lab/C2_W1_Assignment.ipynb)
>>- In this exercise, you will use a neural network to recognize the hand-written digits. You will first learn to build a neural network in a popular machine learning framework - Tensorflow.  You will use your model to do image recognition on a portion of the famous MNIST data set. You will then learn what is 'under the hood' of these frameworks by implementing the forward path of the same network in NumPy in your own mini-framework.

### [Week 2](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course2/Week2)
- Build a neural network to perform multi-class classification of handwritten digits in TensorFlow, using categorical cross-entropy loss functions and the softmax activation.
- Learn where to use different activation functions (ReLu, linear, sigmoid, softmax) in a neural network, depending on the task you want your model to perform.
- Use the advanced “Adam optimizer” to train your model more efficiently.

#### Optional Lab
> #### [1. ReLU activation](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course2/Week2/Optional%20Lab/C2_W2_Relu.ipynb)
> #### [2. Softmax](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course2/Week2/Optional%20Lab/C2_W2_SoftMax.ipynb)
>>- Explore Softmax.
> #### [3. Multiclass](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course2/Week2/Optional%20Lab/C2_W2_Multiclass_TF.ipynb)
>>- Implement a small neural network to perform multiclass classification.

#### Practice Lab
> [Neural Networks for Multiclass classification](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course2/Week2/Practice%20Lab/C2_W2_Assignment.ipynb)
>>- In this exercise, you will extend last weeks assigment to recognize digits 0-9. You will explore the popular ReLu activation as well as the use of the Softmax activation in multiclass classification.  You will then build a neural network capable of multiclass classification using the ReLu and Softmax in Tensorflow. 

### [Week 3](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course2/Week3)
- Discover the value of separating your data set into training, cross-validation, and test sets.
- Choose from various versions of your model using a cross-validation dataset, and evaluate its ability to generalize to real-world data using a test dataset.
- Use “learning curves” to determine if your model is experiencing high bias or high variance (or both), and learn which techniques to apply (regularization, adding more data, adding or removing input features) to improve your model’s performance.
- Learn how the “bias-variance trade-off” is different in the age of deep learning, and apply Andrew Ng’s advice for handling bias and variance when training neural networks.
- Learn to apply the “iterative loop” of machine learning development to train, evaluate, and tune your model.
- Apply “data-centric AI” to not only tune your model but tune your data (using data synthesis or data augmentation) to improve your model’s performance.

#### Practice Lab
> [Advice for Applying Machine Learning](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course2/Week3/C2_W3_Assignment.ipynb)
>>- In this lab, you will explore techniques to evaluate and improve your machine learning models. You will discover how splitting your data set can provide insight into how your model will perform in a production environment and guidance on how to improve your model.

### [Week 4](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course2/Week4)
- Build decision trees and tree ensembles, such as random forest and XGBoost (boosted trees) to make predictions.
- Learn when to use neural network or tree ensemble models for your task, as these are the two most commonly used supervised learning models in practice today.

#### Practice Lab
> [Decision Trees](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course2/Week4/C2_W4_Decision_Tree_with_Markdown.ipynb)
>>- In this exercise, you will implement a decision tree from scratch and apply it to the task of classifying whether a mushroom is edible or poisonous.

## [Course 3: Unsupervised Learning, Recommenders, Reinforcement Learning](https://www.coursera.org/learn/unsupervised-learning-recommenders-reinforcement-learning?specialization=machine-learning-introduction)
In the third course of the Machine Learning Specialization, you will: Use unsupervised learning techniques for unsupervised learning: including clustering and anomaly detection. Build recommender systems with a collaborative filtering approach and a content-based deep…

### In the third course of the Machine Learning Specialization, you will:
- Use unsupervised learning techniques for unsupervised learning: including clustering and anomaly detection.
- Build recommender systems with a collaborative filtering approach and a content-based deep learning method.
- Build a deep reinforcement learning model.

### [Week 1](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course3/Week1)
- Implement K-mean clustering.
- Implement anomaly detection.
- Learn how to choose between supervised learning or anomaly detection to solve certain tasks.

#### Practice Lab
> #### [1. k-means](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course3/Week1/Practice%20Lab1/C3_W1_KMeans_Assignment.ipynb)
>>- In this practice lab, you'll implement the k-means clustering algorithm and use it for image compression!
> #### [2. Anomaly Detection](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course3/Week1/Practice%20Lab2/C3_W1_Anomaly_Detection.ipynb)
>>- In this practice lab, you will implement the anomaly detection algorithm and apply it to monitor computer servers to identify potentially failing (anomalous) servers.

### [Week 2](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course3/Week2)
- Build a recommender system using collaborative filtering.
- Build a recommender system using a content-based deep learning method.

#### Practice Lab
> #### [1. Collaborative Filtering Recommender Systems](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course3/Week2/Practice%20Lab1/C3_W2_Collaborative_RecSys_Assignment.ipynb)
>>- In this lab, you will build a collaborative filtering recommender system for recommending movies using Tensorflow.
> #### [2. Deep Learning for Content-Based Filtering](https://github.com/kawamura-R/Machine-Learning-Specialization/blob/main/Course3/Week2/Practice%20Lab2/C3_W2_RecSysNN_Assignment.ipynb)
>>- In this lab, you will implement a content-based collaborative filtering recommender system for movies. This lab will use neural networks to generate the user and movie vectors.

### [Week 3](https://github.com/kawamura-R/Machine-Learning-Specialization/tree/main/Course3/Week3)
- Build a deep reinforcement learning model (Deep Q Network).
